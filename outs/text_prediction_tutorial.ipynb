{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Prediction with Context Tree Switching\n",
    "\n",
    "The Context Tree Switching algorithm (CTS; Veness et al., 2012) is a powerful tool for sequential prediction. While deep learning methods often achieve better accuracy in the limit of data, CTS can be trained in a single pass, needs no hyperparameter tuning, and also executes significantly faster than most current deep learning algorithms. It's also easily ensembled with other models of the same kind (see, e.g., [Partition Tree Weighting](https://arxiv.org/abs/1211.0587)) This notebook and the following (on density modelling) are designed to show basic CTS usage and highlight the algorithm's statistical efficiency.\n",
    "\n",
    "Both [Joel Veness's implementation](http://jveness.info/publications/default.html) and [the original SkipCTS implementation](https://github.com/mgbellemare/SkipCTS) are designed with binary compression in mind. In binary compression, each symbol is either 0 or 1. By contrast, most of modern sequential prediction uses larger alphabets: pixel values, words, ASCII characters, etc. If you're theory-leaning, you may argue that larger alphabets can always be reduced to bits. If you're more practically-minded, you know this isn't a computationally efficient choice.\n",
    "\n",
    "This Python implementation addresses this issue by providing a model which can deal with arbitrary (categorical) alphabets, while simultaneously making the algorithm more accessible. I chose to focus here on usage, rather than delve into the math underlying the CTS magic. You'll find equations to your heart's content by reading [Context Tree Switching](http://arxiv.org/abs/1111.3182) by Veness, Ng, Hutter, and Bowling (2012)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "\n",
    "from cts import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Text Prediction\n",
    "\n",
    "Although CTS truly shines when used for binary compression, it also does an excellent job at predicting text. We will begin by training our model on [Alice in Wonderland by Lewis Carroll](http://www.gutenberg.org/cache/epub/11/pg11.txt). Part of the [Canterbury Corpus](http://corpus.canterbury.ac.nz/), Alice in Wonderland is one of many texts which have been aggressively compressed by generations of graduate students wanting to make their mark in the field of data compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model, this should take about 10-15 seconds...\n",
      "Bytes read: 163810\n",
      "Equivalent compressed size in bytes: 52911.8\n"
     ]
    }
   ],
   "source": [
    "# We will predict by looking back up to 8 characters into the past.\n",
    "alice_model = model.ContextualSequenceModel(context_length=8)\n",
    "\n",
    "def train_model_alice(alice_model):\n",
    "    \"\"\"This method trains a given model on Alice in Wonderland.\"\"\"\n",
    "    print ('Training model, this should take about 10-15 seconds...')\n",
    "    \n",
    "    with open('alice.txt') as fp:\n",
    "        num_characters = 0\n",
    "        alice_log_probability = 0\n",
    "\n",
    "        while True:\n",
    "            character = fp.read(1)\n",
    "            if not character:\n",
    "                break\n",
    "            else:\n",
    "                num_characters += 1\n",
    "\n",
    "            # model.update() trains the model on one symbol at a time. It returns the log probability of the symbol.\n",
    "            # The negative log probability is the model's loss on this element of the sequence.\n",
    "            symbol_log_probability = alice_model.update(character)\n",
    "            alice_log_probability += symbol_log_probability\n",
    "\n",
    "        print ('Bytes read: {}'.format(num_characters))\n",
    "        print ('Equivalent compressed size in bytes: {:.1f}'.format(-alice_log_probability / math.log(2) / 8))\n",
    "\n",
    "train_model_alice(alice_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Equivalent compressed size* above is the negative log probability (expressed in bytes) of the whole sequence. The really cool thing about compression is that [arithmetic coding](https://en.wikipedia.org/wiki/Arithmetic_coding) guarantees we can compress the original file down to within a bit of this size (modulo headers). In this case we shrink the file size to about 1/3rd of the original.\n",
    "\n",
    "In machine learning terms, the sequence negative log probability is also the loss being minimized. Minimizing this loss is equivalent to maximizing prediction accuracy (for categorical distributions).\n",
    "\n",
    "Context Tree Switching uses context (here, up to the eight most recent characters) to predict the next symbol. To see that this helps minimize the loss, consider the same model with a maximum context length of 0. This corresponds to predicting according to the empirical frequency of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model, this should take about 10-15 seconds...\n",
      "Bytes read: 163810\n",
      "Equivalent compressed size in bytes: 94102.0\n"
     ]
    }
   ],
   "source": [
    "train_model_alice(model.ContextualSequenceModel(context_length=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the negative log probabilities (94102 vs 52911), it's clear that context improves prediction. Amazingly enough, the length 0 model still gets us a 43% compression rate. In this case, the model does something akin to [adaptive Huffman coding](https://en.wikipedia.org/wiki/Adaptive_Huffman_coding), effectively rearranging the English alphabet for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the model.\n",
    "\n",
    "Georg R.R. Martin, of Game of Thrones fame, is well-known for releasing new books ever so slowly. We used to joke around the lab that rather than wait for him to write the next book in the series (which usually takes five or six years), we would be better off training a CTS model to produce it for us. (As of November 2016, we're still waiting on that sixth book.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_sequence(model, num_symbols, rejection_sampling=True):\n",
    "    \"\"\"Samples a sequence from the model.\n",
    "    \n",
    "    Args:\n",
    "        num_symbols: Sequence length.\n",
    "        rejection_sampling: If True, only draw from observed symbols (see below).\n",
    "    \"\"\"\n",
    "    old_context = model.context.copy()\n",
    "    sampled_string = ''\n",
    "    \n",
    "    for t in range(num_symbols):\n",
    "        # Sample a single symbol from the distribution. In this case we use rejection sampling to ignore characters\n",
    "        # we haven't seen (this leads to much nicer text).\n",
    "        symbol = model.sample(rejection_sampling=rejection_sampling)\n",
    "        # observe() moves the model's history one symbol forward, without updating the model parameters.\n",
    "        model.observe(symbol)\n",
    "        sampled_string += symbol\n",
    "    \n",
    "    # Restore the model's context.\n",
    "    model.context = old_context\n",
    "    \n",
    "    return sampled_string    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the sample you're about to see was generated on the fly. It may be benign, genius, or downright offensive. I take no responsibility for the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I muchnessed and a said wasting a to like you glad\n",
      "'tis never you'll into go no!' conden\n",
      "she hers her to\n",
      "The Queen the I fish old, bat, to fair, 'IF I minute or she while tell a dull rem\n",
      "say follown the but thing-mently,' Bill the and Alice on beated, and she was hand said Alice, up and do, flamingoing Alice this sometione using While, few than yourself on watcouldn't both of support said handed,\n"
     ]
    }
   ],
   "source": [
    "print(sample_sequence(alice_model, num_symbols=400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respecting the math.\n",
    "\n",
    "In generating the sequence above we cheated a little: we used rejection sampling to ignore any character which hasn't been observed in the particular context chosen by CTS. Restricting ourselves to observed symbols is a common trick used by sequential models (typically implicitly) to improve the qualitative look of models. In machine learning terms, we're overfitting to the observed alphabet.\n",
    "\n",
    "With CTS, as long as we define the alphabet ahead of time, we can sample from the true posterior. From an online learning perspective, this is an \"honest\" sample: it matches the loss we would suffer from predicting this symbol.\n",
    "\n",
    "See for yourself how sample quality degrades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model, this should take about 10-15 seconds...\n",
      "Bytes read: 163810\n",
      "Equivalent compressed size in bytes: 50866.7\n",
      "Sampling a sequence:\n",
      "\n",
      "Project Gutenberg-tm\n",
      "'Qu?' And a gletVENc:Z-7ew wea. 'MED, I she certainly story, 'jury\"E.1`G#GCa\t\u000b",
      "dkVEc`queLERO`XiJEIu\n",
      "chance.\n",
      "\n",
      "'A be at S{1.zOeQueen?' said el!'\n",
      "\n",
      "   Footman slowl Possional yourse; all NiN=}XtqueKZ#f\n",
      "but#Just to looking I'll expensd-pi*Tt3. 'It in subject should are the March Hare, all rapJust candly sames is the out have all retuN!' said the get\n",
      "because creature said bright it'\n"
     ]
    }
   ],
   "source": [
    "# Since Alice in Wonderland is written in English, we can use string.printable as our alphabet.\n",
    "printable_alphabet = set(string.printable)\n",
    "# Let's make sure there aren't any non-printable characters in Alice in Wonderland.\n",
    "assert(alice_model.model.alphabet.issubset(printable_alphabet))\n",
    "\n",
    "# Create a model with a pre-specified alphabet.\n",
    "core_model = model.CTS(alphabet=printable_alphabet, context_length=8)\n",
    "# The ContextualSequenceModel is really a wrapper around CTS. It keeps track of the history of observed\n",
    "# symbols and uses the most recent as context.\n",
    "alice_model_small_alphabet = model.ContextualSequenceModel(model=core_model)\n",
    "\n",
    "train_model_alice(alice_model_small_alphabet)\n",
    "print ('Sampling a sequence:')\n",
    "print (sample_sequence(alice_model_small_alphabet, num_symbols=400, rejection_sampling=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Changing the prior.\n",
    "\n",
    "Notice also how, when we specify the alphabet, we get better predictive performance: we compress the file in 50866 bytes, rather than 52911 (even though the samples look worse). To see how significant this 4% improvement is, consider that there's good money (in pre-deep learning era dollars) for [compressing 100Mb Wikipedia down by 1 one percent](http://prize.hutter1.net/). A four percent improvement on the Canterbury or [Calgary](https://en.wikipedia.org/wiki/Calgary_corpus) corpuses is probably enough for a best paper award at the [Data Compression Conference](http://www.cs.brandeis.edu/~dcc/).\n",
    "\n",
    "In the spirit of maximizing prediction accuracy, our implementation uses the Perks prior by default. The Perks prior assigns a pseudo-count of $1/A$ to each symbol (in each context), where $A$ is the size of the alphabet. We can change this, for example to the more natural Laplace prior, named after [Pierre-Simon Laplace's rule of succession](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace). The Laplace prior uses a much larger pseudo-count of 1 per symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model, this should take about 10-15 seconds...\n",
      "Bytes read: 163810\n",
      "Equivalent compressed size in bytes: 67039.2\n"
     ]
    }
   ],
   "source": [
    "# Create a model with a pre-specified alphabet.\n",
    "core_model = model.CTS(alphabet=printable_alphabet, context_length=8, symbol_prior='laplace')\n",
    "\n",
    "# The ContextualSequenceModel is really a wrapper around CTS. It keeps track of the history of observed\n",
    "# symbols and uses the most recent as context.\n",
    "alice_model_perks_prior = model.ContextualSequenceModel(model=core_model)\n",
    "train_model_alice(alice_model_perks_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wowza! That's pretty bad. Better not get your priors wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the CTS prior.\n",
    "\n",
    "We conclude this section with a little fun experiment: sampling from the CTS prior over *sequences*. This shows one of the greatest strengths of the CTS algorithm: it adapts very quickly to new data. We'll sample a sequence as before, except now we properly update the model with each sampled symbol.\n",
    "\n",
    "Oh, and also, we won't train the model ahead of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 0:\n",
      "urzj iydnczjya illsuedvwczjya illpcgillpczvillslykluwczjlau iyillfmnczjlmzjrygubslykgwqillsuelzgiy iypbafmndvsdooooucoooucoooooooucohguedophaysquedvrbaryyredrqgedvrncokplqwczgrljzrwnwwzcyrlccgodyghi lolupoioajrlsswlisovzc rdayccneocggydraujddeilonwyljzclgwsslyoocolillmijofgaicuddglqloprsgesrmlisqekgeooriocjiirocqfdobgcooyuzoizvzyyzrqj  zjbdofocowcikloyj  lnvoosdiiejyiacirlycclyiailddyoyloye onpwwl\n",
      "\n",
      "Sequence 1:\n",
      "fak vcnjoxtsmbfsygziwqwluo bfygzalnegvygzdjdwqwqgffwqqnnsqswbxoqsf listngnftvlt fyxm ajquajfoyjzgwtwlkenttfwvqiaigygfiwygvobkwxwwwfqogtawni dowfgogsoiknmgzyfobgt giqzfgiqbwzzwalikvoftvqzgxvomyq ztqtninogasglfgfttovqawwvzgagcfnzfffmosjvgwqwawgbqajygfdyx tfngwwlcfogoa atnwnoycvlovzzzgogtczigqvx ogzgitifo gwswqgew gxzj nleo niwjggoawgngwf ocoqxtczgklf ngayz wlgq w  usytwwnqwwzjxkqlkqvzowvgitogligyezj\n",
      "\n",
      "Sequence 2:\n",
      "kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkxxlvckxxlyryojjcojjkkkkkkkkkkkkkkkkkkkkkxlyegzbvcozbvcvcignbvscvciijcvcvcigazbvcvyegzbvckkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkiikkijgkkkckkkkkibkkkycbikkkkkkkckkkklkkkkkkkcklkkkkskkkkkkksklkkkkkkkxcakkkkvkzkkkkkkkkckkkkkbkjvkkkkikbjkkkkkkiszkkkckkjkkkkkkskkckkkkkkccekckkkkk\n",
      "\n",
      "Sequence 3:\n",
      "jjuuuuuyvpzbjtrzbytrzbytyuyyuyzbrvjubuuytyytuutuzyuvuruutjjtyyuryyyjrjuyjrzbyjyjujuytzjjyrjrtuyrtjujjtyryuybjyuutyytjttzvjbttrrjtjutytjuvjtubjuttuutybyjjjjjurjuttjjjrbrytyyjjjuvytbjtrbrtrttjzzzjtbztjtruvruyjttrtrjruzbtjuruuutuyvyyztrzyrbjujrryjyyvyjvuuuyrujbbjuzjvyuubuuuuyyruvurbbybyjrtzjutjjuzttjjuyrujttuuuujurttjuyyyyrjytvjjyyrryjjurtuyjjyuuuuyvjjztvjyyyyjjuyrjryjjtyyjvjzjyuyjujjjyurtuyuuujutbyj\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's use the letters subset to make the whole thing more legible.\n",
    "sample_alphabet = set(string.ascii_lowercase + ' ')\n",
    "\n",
    "def sample_from_prior():\n",
    "    core_model = model.CTS(alphabet=sample_alphabet, context_length=8, symbol_prior='perks')\n",
    "    prior_model = model.ContextualSequenceModel(model = core_model)\n",
    "\n",
    "    sampled_string = ''\n",
    "\n",
    "    for t in range(400):\n",
    "        symbol = prior_model.sample(rejection_sampling=False)\n",
    "        sampled_string += symbol\n",
    "        # Note: we call update() to adjust the model parameters to the sample.\n",
    "        prior_model.update(symbol)\n",
    "\n",
    "    print (sampled_string)\n",
    "\n",
    "for i in range(4):\n",
    "    print ('Sequence {}:'.format(i))\n",
    "    sample_from_prior()\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'What am I seeing,' you ask? These are the kind of sequences that CTS expects to observe before it sees any data. That is, these are the most likely sequences under the CTS prior, given we may only observe ASCII letters and space. Re-run the above cell a number of times and you'll see similar results. The nice structure heavily depends on the Perks prior, which you can verify by changing to `laplace` or `jeffreys`. You can also set the prior to a floating value (pseudo-count for each symbol; try `0.1 / len(printable_alphabet)`).\n",
    "\n",
    "Sampling from the prior illustrates why the Perks prior does so much better at compressing text than Jeffreys's or Laplace's priors: the samples look more text-like (modulo alphabet permutations). Still, there's a lot of room for improvement! The divergence of these samples from true English (or French, or...) text is exactly the excess cost we suffer for compressing text with CTS.\n",
    "\n",
    "Isn't that cool?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [next tutorial](density_modelling_tutorial.ipynb) demonstrates how to use CTS for modelling images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
